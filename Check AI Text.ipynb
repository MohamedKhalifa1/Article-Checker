{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "10fa95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9f5eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"data/article_level_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "950fc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bda9f8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NLP is a multidisciplinary field that draws fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>There are a variety of emerging applications f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>As each new means of communication and social ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>These suggestions include:, Learn about the pu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In recent years there has been growing concern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            article  class\n",
       "0           0  NLP is a multidisciplinary field that draws fr...      0\n",
       "1           1  There are a variety of emerging applications f...      0\n",
       "2           2  As each new means of communication and social ...      0\n",
       "3           3  These suggestions include:, Learn about the pu...      0\n",
       "4           4  In recent years there has been growing concern...      0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6003cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns=['article','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "099a2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[text_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efa81d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article    0\n",
       "class      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc480e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1018 entries, 0 to 1017\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   article  1018 non-null   object\n",
      " 1   class    1018 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 16.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4206802",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cca99681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i can write this articl'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('i can write this article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61a1770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cleaning(article):\n",
    "    article=re.sub('[^a-zA-Z]',' ',article)\n",
    "    article=article.lower()\n",
    "    article=article.split()\n",
    "    article=[stemmer.stem(sentences)for sentences in article if not sentences in stopwords.words('english')]\n",
    "    article=\" \".join(article)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78d4ff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write articl\n"
     ]
    }
   ],
   "source": [
    "print(word_cleaning('i can write this article'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9296a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article']=df['article'].apply(word_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "543a338a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nlp multidisciplinari field draw linguist comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>varieti emerg applic nlp includ follow voic co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new mean commun social interact introduc socia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suggest includ learn purpos newsgroup post gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recent year grow concern internet user may eve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>palett refer differ thing depend context gener...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>probabl measur likelihood specif event occur m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>compil softwar program translat sourc code wri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>compil process process compil translat sourc c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>code gener process convert high level sourc co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1018 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  class\n",
       "0     nlp multidisciplinari field draw linguist comp...      0\n",
       "1     varieti emerg applic nlp includ follow voic co...      0\n",
       "2     new mean commun social interact introduc socia...      0\n",
       "3     suggest includ learn purpos newsgroup post gro...      0\n",
       "4     recent year grow concern internet user may eve...      0\n",
       "...                                                 ...    ...\n",
       "1013  palett refer differ thing depend context gener...      1\n",
       "1014  probabl measur likelihood specif event occur m...      1\n",
       "1015  compil softwar program translat sourc code wri...      1\n",
       "1016  compil process process compil translat sourc c...      1\n",
       "1017  code gener process convert high level sourc co...      1\n",
       "\n",
       "[1018 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "97e16a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['article']\n",
    "y=df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9309a63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5215ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e6f4dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1f900b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((661,), (357,), (661,), (357,))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "33d848d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4b814165",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.fit_transform(X_train)\n",
    "X_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0b1c8f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((661, 5503), (357, 5503))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5c6f0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "236a5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5b7a5635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "800917b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predction = model.predict(X_test)\n",
    "predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b024b343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7030812324929971"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "20d99501",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vect,open(\"vector.pkl\",\"wb\"))\n",
    "pickle.dump(model,open(\"model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a200a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_form=pickle.load(open('vector.pkl', 'rb'))\n",
    "model = pickle.load(open(\"model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b00bc24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(article):\n",
    "    article = word_cleaning(article)\n",
    "    input_data = [article]\n",
    "    vector_form1= vector_form.transform(input_data)\n",
    "    prediction = model.predict(vector_form1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3f39a683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Human Generated\n"
     ]
    }
   ],
   "source": [
    "val=response(\"\"\"In these trying times, Jackie Mason is the Voice of Reason. [In this week’s exclusive clip for Breitbart News, Jackie discusses the looming threat of North Korea, and explains how President Donald Trump could win the support of the Hollywood left if the U. S. needs to strike first.  “If he decides to bomb them, the whole country will be behind him, because everybody will realize he had no choice and that was the only thing to do,” Jackie says. “Except the Hollywood left. They’ll get nauseous. ” “[Trump] could win the left over, they’ll fall in love with him in a minute. If he bombed them for a better reason,” Jackie explains. “Like if they have no transgender toilets. ” Jackie also says it’s no surprise that Hollywood celebrities didn’t support Trump’s strike on a Syrian airfield this month. “They were infuriated,” he says. “Because it might only save lives. That doesn’t mean anything to them. If it only saved the environment, or climate change! They’d be the happiest people in the world. ” Still, Jackie says he’s got nothing against Hollywood celebs. They’ve got a tough life in this country. Watch Jackie’s latest clip above.   Follow Daniel Nussbaum on Twitter: @dznussbaum \"\"\")\n",
    "print(val)\n",
    "if val==[0]:\n",
    "    print('Human Generated')\n",
    "else:\n",
    "    print('AI Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6e7bce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "AI Generated\n"
     ]
    }
   ],
   "source": [
    "val=response(\"\"\" It appears you've instantiated a TfidfVectorizer object. The TfidfVectorizer is a commonly used tool in natural language processing and text analysis for converting a collection of raw documents into a matrix of TF-IDF features. TF-IDF stands for Term Frequency-Inverse Document Frequency and is used to represent the importance of a word in a document relative to a collection of documents.\n",
    "\n",
    "If you have specific tasks or questions related to the use of TfidfVectorizer or need assistance with using it, please provide more details, and I'd be happy to help.\"\"\")\n",
    "print(val)\n",
    "if val==[0]:\n",
    "    print('Human Generated')\n",
    "else:\n",
    "    print('AI Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "20430a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "AI Generated\n"
     ]
    }
   ],
   "source": [
    "val=response(\"\"\"Certainly, I can help you generate a project name. Here's a project name you can consider:\n",
    "\n",
    "Project Name: \"AIDetect\"\n",
    "\n",
    "You can use this project name to check if responses are generated by AI or not. If the response includes the project name \"AIDetect,\" it's likely generated by AI. This project name is a combination of \"AI\" (Artificial Intelligence) and \"Detect,\" which suggests the project's focus on detecting AI-generated content.\"\"\")\n",
    "print(val)\n",
    "if val==[0]:\n",
    "    print('Human Generated')\n",
    "else:\n",
    "    print('AI Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c4d1685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a prompt engineering NLP (Natural Language Processing) system using Python involves several technical steps. Prompt engineering is the process of designing and optimizing prompts or queries for language models like GPT-3. Here are the steps you should take:  1. **Set Up Your Environment:**    Ensure you have Python installed and set up your development environment. You can use popular Python environments like Anaconda or virtualenv.  2. **Install Required Libraries:**    You'll need various Python libraries, including but not limited to:    - OpenAI's GPT-3 library (e.g., `openai`)    - Natural Language Toolkit (NLTK) or spaCy for text processing    - Flask or FastAPI for building a web interface    - Other data manipulation and visualization libraries as per your requirements  3. **Get API Access:**    To use GPT-3, sign up for OpenAI's API and obtain your API key. Keep this key secure.  4. **Data Collection and Preprocessing:**    Collect and preprocess your data. This might involve scraping, cleaning, and formatting the data you want to use as prompts or training data for GPT-3.  5. **Design Prompts:**    Develop a strategy for prompt engineering. This involves creating prompts that will yield the desired output. Experiment with different prompts and formats to see what works best. Pay attention to the instruction format that GPT-3 understands.  6. **Integration with GPT-3:**    Use the OpenAI API to interact with GPT-3. You can use Python libraries to send requests to the API, passing your prompts and receiving model-generated responses.  7. **Experiment and Iterate:**    Start with simple prompts and gradually make them more complex. Experiment with different parameters, like temperature (controls the randomness of output) and max tokens (limits the length of the response). Fine-tune your prompts based on the model's responses.  8. **Handle Output:**    Process and filter the model's responses as needed. You might want to extract specific information from the generated text or perform sentiment analysis on the output.  9. **Error Handling:**    Implement robust error handling to deal with issues like timeouts or unexpected API responses. You should gracefully handle such errors to maintain a reliable application.  10. **Performance Optimization:**     Optimize the performance of your application. You might need to handle rate limits and manage costs if you're making a high volume of API requests.  11. **Security Considerations:**     Ensure that you're handling sensitive data and API keys securely. Follow best practices for securing your application, especially if it's going to be publicly accessible.  12. **Testing:**     Rigorously test your application. This includes unit testing for prompt designs and integrations, as well as end-to-end testing for your entire system.  13. **Documentation:**     Create documentation for your prompt engineering process, how to use your system, and any considerations users should be aware of.  14. **Deployment:**     Deploy your application to a server or cloud platform. Make sure it's accessible to users as needed.  15. **Monitoring and Maintenance:**     Continuously monitor your system's performance and maintain it. Keep an eye on the accuracy and reliability of the responses and make updates as necessary.  Remember that prompt engineering is often an iterative process, and you'll likely need to fine-tune your prompts and system over time to improve the quality of the responses and achieve your NLP goals.\n",
      "[1]\n",
      "AI Generated\n"
     ]
    }
   ],
   "source": [
    "res=input()\n",
    "val=response(res)\n",
    "print(val)\n",
    "if val==[0]:\n",
    "    print('Human Generated')\n",
    "else:\n",
    "    print('AI Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d55eaa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring the quiet charm of a small coastal town is a delightful experience. The gentle sound of waves lapping against the shore, the salty breeze in the air, and the quaint, colorful cottages that line the streets create an idyllic setting for a relaxing getaway. As you stroll through the town, you can't help but notice the friendly smiles of the locals, who take pride in their tight-knit community.  The town's seafood market is a must-visit, where you can savor the freshest catches of the day. Whether it's succulent shrimp, buttery lobster, or flaky cod, you'll find a variety of options to satisfy your taste buds. Don't forget to pair your meal with a chilled glass of white wine from a local vineyard for the perfect coastal dining experience.  One of the town's hidden gems is a historic lighthouse that stands proudly at the edge of the rocky cliffs. Climbing to the top offers breathtaking panoramic views of the ocean, with seagulls soaring in the distance. It's a spot where you can lose track of time, watching the sun dip below the horizon, painting the sky in shades of pink and orange.  Evenings in the town are a time for camaraderie. The local pub comes alive with live music, and it's easy to strike up a conversation with fellow travelers and residents alike. The stories that are shared, the laughter that fills the air, and the sense of belonging make you feel like you've discovered a little piece of paradise.  In this charming coastal town, time seems to slow down, allowing you to appreciate the simple pleasures of life and the beauty of nature. It's a place where you can escape from the hustle and bustle of the modern world and embrace the serenity of the sea.\n",
      "[0]\n",
      "Human Generated\n"
     ]
    }
   ],
   "source": [
    "res=input()\n",
    "val=response(res)\n",
    "print(val)\n",
    "if val==[0]:\n",
    "    print('Human Generated')\n",
    "else:\n",
    "    print('AI Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ff6b641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Mohamed Ashraf! It's a pleasure to have the opportunity to connect with you. Your name carries with it a sense of uniqueness and individuality, and I'm here to assist and engage with you in any way you need. Whether you have questions, seek information, or just want a friendly chat, feel free to reach out. Your name is your identity, and I'm here to make your experience enjoyable and informative. Don't hesitate to share your thoughts, questions, or interests, and I'll do my best to assist you. Welcome!\n",
      "[1]\n",
      "AI Generated\n"
     ]
    }
   ],
   "source": [
    "res=input()\n",
    "val=response(res)\n",
    "print(val)\n",
    "if val==[0]:\n",
    "    print('Human Generated')\n",
    "else:\n",
    "    print('AI Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "30ad3b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Mohamed Ashraf! It's wonderful to meet you. Your name has a special meaning, and it's a pleasure to have this moment to connect with you. How can I assist you today? Whether it's answering questions, sharing stories, or just having a friendly conversation, I'm here for you. Feel free to open up and share your thoughts, and let's make our interaction enjoyable and meaningful. Welcome, and let's get started\n",
      "[0]\n",
      "Human Generated\n"
     ]
    }
   ],
   "source": [
    "res=input()\n",
    "val=response(res)\n",
    "print(val)\n",
    "if val==[0]:\n",
    "    print('Human Generated')\n",
    "else:\n",
    "    print('AI Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f18839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
